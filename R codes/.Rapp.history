df_full<-read.csv("/Users/sajal/UPENN/independent_study/arranged_data/final_dataset.csv",header=T) #
exclude_list <- c(1,3,17,31,45,52,59,72,82,105,106,129,144,193,245,451,456,1182) #Excluding whose essays are not present.#
df_full <- df_full[-exclude_list,]#
sub <- df_full[, c(5,6,7,8,10,11,12,13,14,15)]#
cor_d <- cor(sub)#
library(reshape2)#
library(ggplot2)#
library(gplots)#
library(glmnet)#
library(lattice) #
melted_cor <- melt(cor_d)#
ggplot(data = melted_cor, aes(x=Var1, y=Var2, fill=value)) + geom_tile()#
qplot(x=Var1, y=Var2, data=melted_cor, fill=value, geom="tile") + scale_fill_gradient2(limits=c(-1, 1))#
#
dev.off()#
heatmap.2(cor_d,dendrogram = "none", Rowv = FALSE, Colv = FALSE, symm = TRUE,trace='both',nrow=10)#
#Code for regression starts#
#
machine_df <-df_full[,c(6,7,8,10,11)]#
human_df <-df_full[,c(12,13,14,15)]#
#
Y <- as.data.frame(df_full$Wharton.GPA_To.Date)#
#
#***Should be common to all the models below ***##
set.seed(100)#
row_len<-nrow(df_full)#
indices <- sample(row_len,1000,replace = FALSE) #Get 1000 random indices #
#***Should be common to all the models below ***##
mac_train <- machine_df[indices,]#
mac_test <- machine_df[-indices,]#
#
human_train <- human_df[indices,]#
human_test <- human_df[-indices,]#
#
trainY <- Y[indices,]#
testY <- Y[-indices,]#
#
#BaseLine#
mse_avg <- sqrt(mean((testY - rep(mean(testY),559))^2))#
#
#Ridge Machine#
xmat_tr <- data.matrix(mac_train)#
ridge_model <- cv.glmnet(xmat_tr,trainY, alpha = 0) #
#
rr.cv <- cv.glmnet(xmat_tr, trainY, alpha = 0)#
plot(rr.cv) #Plotting variation of error with change of lambda (Regularization parameter)#
rr.bestlam <- rr.cv$lambda.min#
plot(ridge_model$glmnet.fit,xvar="lambda",label=TRUE) #Plot for coffecients of variables vs log lambda#
#
xmat_test <- data.matrix(mac_test)#
predict_ridge <- predict(ridge_model, xmat_test, type ="response", s=rr.bestlam)[,1]#
mse_machine_ridge <- sqrt(mean((predict_ridge - testY)^2))#
#Lasso Machine#
#
xmat_tr <- data.matrix(mac_train)#
ridge_model <- cv.glmnet(xmat_tr,trainY, alpha = 1) #
#
rr.cv <- cv.glmnet(xmat_tr, trainY, alpha = 1)#
plot(rr.cv) #Plotting variation of error with change of lambda (Regularization parameter)#
rr.bestlam <- rr.cv$lambda.min#
plot(ridge_model$glmnet.fit,xvar="lambda",label=TRUE) #Plot for coffecients of variables vs log lambda#
#
xmat_test <- data.matrix(mac_test)#
predict_lasso <- predict(ridge_model, xmat_test, type ="response", s=rr.bestlam)[,1]#
mse_machine_lasso <- sqrt(mean((predict_lasso - testY)^2))#
#Elastic Machine#
#
xmat_tr <- data.matrix(mac_train)#
ridge_model <- cv.glmnet(xmat_tr,trainY, alpha = 0.5) #
#
rr.cv <- cv.glmnet(xmat_tr, trainY, alpha = 0.5)#
plot(rr.cv) #Plotting variation of error with change of lambda (Regularization parameter)#
rr.bestlam <- rr.cv$lambda.min#
plot(ridge_model$glmnet.fit,xvar="lambda",label=TRUE) #Plot for coffecients of variables vs log lambda#
#
xmat_test <- data.matrix(mac_test)#
predict_elastic <- predict(ridge_model, xmat_test, type ="response", s=rr.bestlam)[,1]#
mse_machine_elastic <- sqrt(mean((predict_elastic - testY)^2))#
#Ridge Human#
#
xmat_tr <- data.matrix(human_train)#
ridge_model <- cv.glmnet(xmat_tr,trainY, alpha = 0) #
#
rr.cv <- cv.glmnet(xmat_tr, trainY, alpha = 0)#
plot(rr.cv) #Plotting variation of error with change of lambda (Regularization parameter)#
rr.bestlam <- rr.cv$lambda.min#
#
xmat_test <- data.matrix(human_test)#
predict_ridge_human <- predict(ridge_model, xmat_test, type ="response", s=rr.bestlam)[,1]#
mse_human_ridge <- sqrt(mean((predict_ridge_human - testY)^2))#
#Compare#
compareTest <- data.frame(TrueLabels = testY, MachinePredict = predict_ridge, MachinePredictLasso = predict_lasso, MachinePredictElastic = predict_elastic, HumanPredict = predict_ridge_human)#
#
xyplot(testY[1:100]+rep(mean(testY),100)+predict_ridge[1:100]+predict_lasso[1:100] + predict_ridge_human[1:100]~c(1:100),type="l",auto.key = T) #First 100 test X values and prediction#
#Prediction through essay words#
#
#Normalization word count by square root of count and adding extra column for normalizing constant#
df_words<-read.csv("/Users/sajal/UPENN/independent_study/arranged_data/word_feature.csv",header=T) #
df_words <- df_words[,c(2:2001)] #Removing col of appids#
df_words <- df_words[-exclude_list,]#
df_mat <- as.matrix(df_words)#
df_sqmat <- sqrt(df_mat)#
normal_c <- rowSums(df_sqmat)#
df_norm <- df_sqmat/normal_c#
df_norm<-cbind(df_norm, normal_c) #Adding normalization constant as an extra column.#
mod_df_words <- as.data.frame(df_norm)#
#data from previously defined train and test indices#
word_train <- mod_df_words[indices,]#
word_test <- mod_df_words[-indices,]#
#
#Ridge regression in words#
xmat_tr <- data.matrix(word_train)#
ridge_model_words <- cv.glmnet(xmat_tr,trainY, alpha = 0) #
#
rr.cv <- cv.glmnet(xmat_tr, trainY, alpha = 0)#
plot(rr.cv) #Plotting variation of error with change of lambda (Regularization parameter)#
rr.bestlam <- rr.cv$lambda.min#
plot(ridge_model_words$glmnet.fit,xvar="lambda",label=TRUE) #Plot for coffecients of variables vs log lambda#
#
xmat_test <- data.matrix(word_test)#
predict_ridge_words <- predict(ridge_model_words, xmat_test, type ="response", s=rr.bestlam)[,1]#
mse_words_ridge <- sqrt(mean((predict_ridge_words - testY)^2))#
#Ensemble predicted with words and predicted with machine attributes#
ensemble_predict <- (predict_ridge_words+predict_ridge)/2#
mse_ensemble <- sqrt(mean((predict_ridge_words - testY)^2))
